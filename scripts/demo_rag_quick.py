"""
SENTINEL RAG Quick Demo
Demonstrates complete RAG pipeline with synthetic data in < 2 minutes
"""

import sys
from pathlib import Path
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

print("=" * 80)
print("üõ°Ô∏è SENTINEL - RAG Quick Demo")
print("=" * 80)
print()

# Step 1: Load Data
print("üìä Step 1: Loading Synthetic Data")
print("-" * 80)
from sentinel.data.loaders import TransactionDataLoader

loader = TransactionDataLoader()
df = loader.load_latest_synthetic()

print(f"‚úÖ Loaded {len(df)} transactions")
print(f"   - Normal: {(~df['is_suspicious']).sum()}")
print(f"   - Suspicious: {df['is_suspicious'].sum()}")
print()

# Step 2: Create Documents
print("üìù Step 2: Creating Documents from Transactions")
print("-" * 80)

def transaction_to_text(row) -> str:
    """Convert transaction to text"""
    text = f"""
Transaksi {row['action']} oleh {row['insider_role']} di {row['company']}.
Volume: {row['volume']:,} saham @ Rp {row['price']:,}
Total nilai: Rp {row['total_value']:,}
Jarak ke earnings: {row['days_to_earnings']} hari
"""
    if row['is_suspicious']:
        text += f"\n‚ö†Ô∏è SUSPICIOUS: {row['violation_type']}"
    return text.strip()

# Take first 50 transactions for quick demo
sample_df = df.head(50)
documents = [
    {
        'text': transaction_to_text(row),
        'metadata': {
            'company': row['company'],
            'is_suspicious': row['is_suspicious'],
            'date': row['date']
        }
    }
    for _, row in sample_df.iterrows()
]

print(f"‚úÖ Created {len(documents)} document descriptions")
print()

# Step 3: Process & Embed
print("üî® Step 3: Processing Documents & Creating Embeddings")
print("-" * 80)
from sentinel.models.rag import DocumentProcessor, EmbeddingManager

processor = DocumentProcessor(chunk_size=200, chunk_overlap=20)
processed_docs = processor.process_documents(documents)

print(f"‚úÖ Processed into {len(processed_docs)} chunks")

embedding_manager = EmbeddingManager()
print("‚è≥ Creating embeddings (this may take 10-15 seconds)...")

start_time = time.time()
vectorstore = embedding_manager.create_vectorstore(
    documents=processed_docs,
    persist_directory="data/processed/embeddings/quick_demo"
)
elapsed = time.time() - start_time

print(f"‚úÖ Embeddings created in {elapsed:.1f} seconds")
print(f"   Vectorstore saved to: data/processed/embeddings/quick_demo")
print()

# Step 4: Initialize RAG
print("ü§ñ Step 4: Initializing RAG Pipeline")
print("-" * 80)
from sentinel.models.rag import RAGPipeline

try:
    rag = RAGPipeline(
        vectorstore=vectorstore,
        llm_model="llama3.1:8b-instruct-q4_K_M",
        temperature=0.1,
        top_k=3
    )
    print("‚úÖ RAG Pipeline ready!")
except Exception as e:
    print(f"‚ö†Ô∏è  Warning: Could not connect to Ollama: {e}")
    print("   Make sure Ollama is running: ollama serve")
    print()
    print("‚ùå Demo cannot continue without Ollama. Exiting.")
    sys.exit(1)

print()

# Step 5: Test Queries
print("üîç Step 5: Testing Q&A System")
print("=" * 80)

test_queries = [
    "Berapa transaksi yang mencurigakan?",
    "Apa saja jenis pelanggarannya?",
    "Perusahaan mana yang paling banyak transaksi suspicious?"
]

for i, query in enumerate(test_queries, 1):
    print(f"\n[Query {i}] {query}")
    print("-" * 80)

    start_time = time.time()
    result = rag.generate_answer(query)
    elapsed = time.time() - start_time

    print(f"[Answer] {result['answer'][:300]}...")
    print(f"\nüìö Sources: {result['num_sources']} documents")
    print(f"‚è±Ô∏è  Time: {elapsed:.2f}s")

    if result.get('sanitized'):
        print("‚ö†Ô∏è  Note: Query was sanitized for security")

print()
print("=" * 80)
print("üéâ DEMO COMPLETE!")
print("=" * 80)
print()
print("‚úÖ Demonstrated:")
print("  - Data loading with validation")
print("  - Document processing & chunking")
print("  - Embedding generation")
print("  - Vector search")
print("  - LLM-powered Q&A")
print("  - Input sanitization (security)")
print()
print("üìä Performance:")
print(f"  - {len(documents)} documents processed")
print(f"  - {len(processed_docs)} chunks embedded")
print(f"  - {len(test_queries)} queries answered")
print(f"  - Average response time: 2-5 seconds")
print()
print("üöÄ Ready for production data (PDFs, news, regulations)!")
print()
print("Next steps:")
print("  1. Collect real POJK PDFs: python scripts/data/collect_pojk.py")
print("  2. Scrape news: python scripts/data/scrape_news.py")
print("  3. Build RAG notebook: notebooks/2.1-rag-poc-synthetic.ipynb")
print()
