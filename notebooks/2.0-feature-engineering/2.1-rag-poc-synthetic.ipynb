{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ SENTINEL - RAG Proof of Concept with Synthetic Data\n",
                "\n",
                "**Objective**: Demonstrate working RAG pipeline for insider trading compliance\n",
                "\n",
                "**What we'll build:**\n",
                "1. Load synthetic transaction data\n",
                "2. Create text descriptions from transactions\n",
                "3. Generate embeddings and store in ChromaDB\n",
                "4. Build Q&A system using Ollama LLM\n",
                "5. Evaluate retrieval quality\n",
                "6. Log experiment to MLflow\n",
                "\n",
                "**Success Criteria:**\n",
                "- Retrieval precision@5 > 80%\n",
                "- Processing time < 5 seconds/query\n",
                "- Answers are relevant and accurate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard library\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(str(Path.cwd().parent.parent / 'src'))\n",
                "\n",
                "# Data science\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Our modules\n",
                "from sentinel.data.loaders import TransactionDataLoader\n",
                "from sentinel.data.validation import validate_transaction_data\n",
                "from sentinel.models.rag import DocumentProcessor, EmbeddingManager, RAGPipeline\n",
                "\n",
                "# MLflow tracking\n",
                "import mlflow\n",
                "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
                "mlflow.set_experiment(\"rag-poc-synthetic\")\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")\n",
                "print(f\"üìÅ Working directory: {Path.cwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Step 1: Load Synthetic Transaction Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data using our professional loader\n",
                "loader = TransactionDataLoader()\n",
                "df = loader.load_latest_synthetic()\n",
                "\n",
                "print(f\"Loaded {len(df)} transactions\")\n",
                "print(f\"\\nData shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")\n",
                "\n",
                "# Display sample\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate data quality\n",
                "validated_df = validate_transaction_data(df)\n",
                "\n",
                "# Summary statistics\n",
                "print(\"üìä Data Summary:\")\n",
                "print(f\"Normal transactions: {(~df['is_suspicious']).sum()} ({(~df['is_suspicious']).mean()*100:.1f}%)\")\n",
                "print(f\"Suspicious transactions: {df['is_suspicious'].sum()} ({df['is_suspicious'].mean()*100:.1f}%)\")\n",
                "print(f\"\\nViolation types:\")\n",
                "print(df[df['is_suspicious']]['violation_type'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Step 2: Create Text Descriptions from Transactions\n",
                "\n",
                "Convert structured transaction data into natural language descriptions for RAG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def transaction_to_text(row) -> str:\n",
                "    \"\"\"Convert transaction row to natural language description\"\"\"\n",
                "    \n",
                "    text = f\"\"\"\n",
                "Transaksi {row['action']} oleh {row['insider_role']} di perusahaan {row['company']}.\n",
                "Nama insider: {row['insider_name']}\n",
                "Tanggal transaksi: {row['date']}\n",
                "Volume: {row['volume']:,} saham\n",
                "Harga: Rp {row['price']:,}\n",
                "Total nilai: Rp {row['total_value']:,}\n",
                "Jarak ke pengumuman earnings: {row['days_to_earnings']} hari\n",
                "\"\"\"\n",
                "    \n",
                "    if row['is_suspicious']:\n",
                "        text += f\"\"\"\n",
                "‚ö†Ô∏è STATUS: SUSPICIOUS\n",
                "Jenis pelanggaran: {row['violation_type']}\n",
                "Alasan: {row.get('reason', 'N/A')}\n",
                "\"\"\"\n",
                "    else:\n",
                "        text += \"‚úÖ STATUS: NORMAL\\n\"\n",
                "    \n",
                "    return text.strip()\n",
                "\n",
                "# Create text descriptions\n",
                "df['text_description'] = df.apply(transaction_to_text, axis=1)\n",
                "\n",
                "# Show sample\n",
                "print(\"Sample text description:\")\n",
                "print(\"=\" * 60)\n",
                "print(df['text_description'].iloc[0])\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî® Step 3: Process Documents & Create Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize document processor\n",
                "doc_processor = DocumentProcessor(\n",
                "    chunk_size=300,  # Smaller chunks for transaction descriptions\n",
                "    chunk_overlap=50\n",
                ")\n",
                "\n",
                "# Prepare documents\n",
                "documents = [\n",
                "    {\n",
                "        'text': row['text_description'],\n",
                "        'metadata': {\n",
                "            'transaction_id': row['transaction_id'],\n",
                "            'company': row['company'],\n",
                "            'is_suspicious': row['is_suspicious'],\n",
                "            'action': row['action'],\n",
                "            'date': row['date']\n",
                "        }\n",
                "    }\n",
                "    for _, row in df.iterrows()\n",
                "]\n",
                "\n",
                "# Process into chunks\n",
                "processed_docs = doc_processor.process_documents(documents)\n",
                "\n",
                "print(f\"‚úÖ Processed {len(documents)} transactions into {len(processed_docs)} chunks\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize embedding manager\n",
                "embedding_manager = EmbeddingManager(\n",
                "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
                ")\n",
                "\n",
                "# Create vectorstore\n",
                "vectorstore_path = \"../../data/processed/embeddings/rag_poc_synthetic\"\n",
                "\n",
                "print(\"Creating embeddings and vectorstore...\")\n",
                "start_time = time.time()\n",
                "\n",
                "vectorstore = embedding_manager.create_vectorstore(\n",
                "    documents=processed_docs,\n",
                "    persist_directory=vectorstore_path\n",
                ")\n",
                "\n",
                "elapsed_time = time.time() - start_time\n",
                "print(f\"‚úÖ Vectorstore created in {elapsed_time:.2f} seconds\")\n",
                "print(f\"üìç Saved to: {vectorstore_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ Step 4: Initialize RAG Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize RAG pipeline with Ollama\n",
                "rag = RAGPipeline(\n",
                "    vectorstore=vectorstore,\n",
                "    llm_model=\"llama3.1:8b-instruct-q4_K_M\",\n",
                "    temperature=0.1,\n",
                "    top_k=5\n",
                ")\n",
                "\n",
                "print(\"‚úÖ RAG Pipeline ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Step 5: Test Q&A System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test queries\n",
                "test_queries = [\n",
                "    \"Berapa banyak transaksi suspicious yang ditemukan?\",\n",
                "    \"Apa saja jenis pelanggaran yang terdeteksi?\",\n",
                "    \"Perusahaan mana yang paling banyak memiliki transaksi suspicious?\",\n",
                "    \"Apa itu quiet period violation?\",\n",
                "    \"Transaksi apa yang dilakukan dekat dengan pengumuman earnings?\"\n",
                "]\n",
                "\n",
                "print(\"üîç Testing RAG Q&A System:\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for i, query in enumerate(test_queries, 1):\n",
                "    print(f\"\\n[Query {i}] {query}\")\n",
                "    \n",
                "    start_time = time.time()\n",
                "    result = rag.generate_answer(query)\n",
                "    elapsed = time.time() - start_time\n",
                "    \n",
                "    print(f\"\\n[Answer] {result['answer']}\")\n",
                "    print(f\"\\nüìö Sources: {result['num_sources']} documents | ‚è±Ô∏è Time: {elapsed:.2f}s\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Step 6: Evaluate Retrieval Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test retrieval accuracy\n",
                "print(\"üß™ Evaluating retrieval quality...\\n\")\n",
                "\n",
                "# Sample queries to test semantic search\n",
                "eval_queries = [\n",
                "    \"transaksi mencurigakan\",\n",
                "    \"quiet period\",\n",
                "    \"volume tidak normal\",\n",
                "    \"pelanggaran insider trading\"\n",
                "]\n",
                "\n",
                "for query in eval_queries:\n",
                "    docs = rag.retrieve_documents(query)\n",
                "    \n",
                "    print(f\"Query: '{query}'\")\n",
                "    print(f\"Retrieved {len(docs)} documents:\")\n",
                "    \n",
                "    for i, doc in enumerate(docs[:3], 1):  # Show top 3\n",
                "        print(f\"  {i}. Company: {doc.metadata.get('company', 'N/A')}, \"\n",
                "              f\"Suspicious: {doc.metadata.get('is_suspicious', 'N/A')}\")\n",
                "        print(f\"     Preview: {doc.page_content[:100]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Step 7: Log Experiment to MLflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Log experiment\n",
                "with mlflow.start_run(run_name=\"rag-poc-synthetic-v1\"):\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_param(\"embedding_model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "    mlflow.log_param(\"llm_model\", \"llama3.1:8b-instruct-q4_K_M\")\n",
                "    mlflow.log_param(\"chunk_size\", 300)\n",
                "    mlflow.log_param(\"chunk_overlap\", 50)\n",
                "    mlflow.log_param(\"top_k\", 5)\n",
                "    mlflow.log_param(\"num_transactions\", len(df))\n",
                "    mlflow.log_param(\"num_documents\", len(processed_docs))\n",
                "    \n",
                "    # Log metrics\n",
                "    mlflow.log_metric(\"suspicious_ratio\", df['is_suspicious'].mean())\n",
                "    mlflow.log_metric(\"embedding_time_sec\", elapsed_time)\n",
                "    \n",
                "    # Log data info\n",
                "    mlflow.log_dict(\n",
                "        {\n",
                "            \"violation_distribution\": df[df['is_suspicious']]['violation_type'].value_counts().to_dict(),\n",
                "            \"company_distribution\": df['company'].value_counts().head(10).to_dict()\n",
                "        },\n",
                "        \"data_distribution.json\"\n",
                "    )\n",
                "    \n",
                "    print(\"‚úÖ Experiment logged to MLflow\")\n",
                "    print(f\"üìä View at: http://localhost:5000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Summary & Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"üéâ RAG POC COMPLETE!\")\n",
                "print(\"=\" * 80)\n",
                "print()\n",
                "print(\"‚úÖ Achievements:\")\n",
                "print(f\"  - Processed {len(df)} transactions\")\n",
                "print(f\"  - Created {len(processed_docs)} document chunks\")\n",
                "print(f\"  - Built working RAG pipeline\")\n",
                "print(f\"  - Embeddings stored in ChromaDB\")\n",
                "print(f\"  - Experiment logged to MLflow\")\n",
                "print()\n",
                "print(\"üìà Next Steps:\")\n",
                "print(\"  1. Collect real POJK PDFs (Week 1)\")\n",
                "print(\"  2. Scrape news articles (Week 1)\")\n",
                "print(\"  3. Build RAG with real documents (Week 2)\")\n",
                "print(\"  4. Improve retrieval accuracy (Week 2)\")\n",
                "print(\"  5. Deploy as API (Week 3)\")\n",
                "print()\n",
                "print(\"üõ°Ô∏è SENTINEL Foundation is SOLID! üöÄ\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
